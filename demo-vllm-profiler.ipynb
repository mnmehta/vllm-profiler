{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Profiler Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow of the vLLM profiler system using a Kubernetes mutating admission webhook.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The vLLM profiler system automatically instruments vLLM pods with PyTorch profiler capabilities:\n",
    "\n",
    "1. **Mutating Webhook** intercepts pod creation\n",
    "2. **Injects profiler code** via ConfigMap and environment variables\n",
    "3. **Auto-loads profiler** when Python starts using `sitecustomize.py`\n",
    "4. **Instruments vLLM** using import hooks to wrap `Worker.execute_model`\n",
    "5. **Captures traces** of CPU+CUDA activity and exports profiler output\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "This demo follows the same workflow as `test-vllm-integration.sh`:\n",
    "\n",
    "1. Check prerequisites\n",
    "2. Deploy the profiler webhook and ConfigMap\n",
    "3. Create a vLLM pod with profiler instrumentation\n",
    "4. Wait for vLLM server to start\n",
    "5. Send inference request generating 200 tokens\n",
    "6. Verify profiler output in logs\n",
    "7. Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Prerequisites\n",
    "\n",
    "Verify we have access to the Kubernetes cluster and required namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "NAMESPACE = \"downstream-llm-d\"\n",
    "POD_NAME = \"demo-vllm-profiler\"\n",
    "MODEL = \"facebook/opt-125m\"  # Small model for testing\n",
    "VLLM_IMAGE = \"vllm/vllm-openai:latest\"\n",
    "MAX_MODEL_LEN = \"2048\"\n",
    "\n",
    "os.environ[\"KUBECONFIG\"] = \"/home/michey/llmd_aug2025/kubeconfig.llmd.8xh200\"\n",
    "os.environ[\"NAMESPACE\"] = NAMESPACE\n",
    "os.environ[\"POD_NAME\"] = POD_NAME\n",
    "os.environ[\"MODEL\"] = MODEL\n",
    "os.environ[\"VLLM_IMAGE\"] = VLLM_IMAGE\n",
    "os.environ[\"MAX_MODEL_LEN\"] = MAX_MODEL_LEN\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Namespace: {NAMESPACE}\")\n",
    "print(f\"  Pod name: {POD_NAME}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Image: {VLLM_IMAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KUBECONFIG\"] = \"/home/michey/llmd_aug2025/kubeconfig.llmd.8xh200\"\n",
    "os.environ[\"NAMESPACE\"] = \"downstream-llm-d\"\n",
    "os.environ[\"POD_NAME\"] = \"demo-vllm-profiler\"\n",
    "os.environ[\"MODEL\"] = \"facebook/opt-125m\"  # Small model for testing\n",
    "os.environ[\"VLLM_IMAGE\"] = \"vllm/vllm-openai:latest\"\n",
    "os.environ[\"MAX_MODEL_LEN\"] = \"2048\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster access\n",
    "!oc whoami\n",
    "!oc cluster-info | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if namespace exists\n",
    "!oc get namespace {NAMESPACE} 2>/dev/null || echo \"Namespace {NAMESPACE} not found - will be created during deployment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deploy the Profiler\n",
    "\n",
    "Deploy the mutating webhook and ConfigMap containing the profiler code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy profiler (this runs deploy.sh)\n",
    "!./deploy.sh --skip-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify webhook is running\n",
    "!oc get pods -n vllm-profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check webhook configuration\n",
    "!oc get mutatingwebhookconfiguration env-injector-webhook -o jsonpath='{.webhooks[0].clientConfig.service.name}' && echo \" (webhook service)\"\n",
    "!echo \"Target namespace: $(oc get deployment env-injector -n vllm-profiler -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name==\"TARGET_NAMESPACE\")].value}')\"\n",
    "!echo \"Target labels: $(oc get deployment env-injector -n vllm-profiler -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name==\"TARGET_LABELS\")].value}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc get configmap env-injector-files -n \"$NAMESPACE\"\n",
    "echo \"ConfigMap contains:\"\n",
    "oc get configmap env-injector-files -n \"$NAMESPACE\" -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create vLLM Pod\n",
    "\n",
    "Create a vLLM pod with the label `llm-d.ai/inferenceServing=true` so the webhook will instrument it.\n",
    "\n",
    "The webhook will automatically:\n",
    "- Inject `PYTHONPATH=/home/vllm/profiler`\n",
    "- Mount `sitecustomize.py` and `profiler_config.yaml` from ConfigMap\n",
    "- Set `VLLM_RPC_TIMEOUT=1800000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, clean up any existing test pod\n",
    "!oc delete pod {POD_NAME} -n {NAMESPACE} --ignore-not-found=true --wait=false\n",
    "!sleep 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$POD_NAME\" \"$NAMESPACE\" \"$VLLM_IMAGE\" \"$MODEL\" \"$MAX_MODEL_LEN\"\n",
    "# Create vLLM pod with profiler instrumentation\n",
    "cat <<EOF | oc apply -f -\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: $1\n",
    "  namespace: $2\n",
    "  labels:\n",
    "    llm-d.ai/inferenceServing: \"true\"\n",
    "    demo: \"vllm-profiler\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: vllm\n",
    "    image: $3\n",
    "    env:\n",
    "    - name: HOME\n",
    "      value: /tmp\n",
    "    - name: HF_HOME\n",
    "      value: /tmp/huggingface\n",
    "    - name: TRANSFORMERS_CACHE\n",
    "      value: /tmp/huggingface\n",
    "    - name: XDG_CACHE_HOME\n",
    "      value: /tmp/cache\n",
    "    - name: FLASHINFER_WORKSPACE_DIR\n",
    "      value: /tmp/flashinfer\n",
    "    command:\n",
    "    - python3\n",
    "    - -m\n",
    "    - vllm.entrypoints.openai.api_server\n",
    "    - --model\n",
    "    - $4\n",
    "    - --max-model-len\n",
    "    - \"$5\"\n",
    "    - --host\n",
    "    - \"0.0.0.0\"\n",
    "    - --port\n",
    "    - \"8000\"\n",
    "    ports:\n",
    "    - containerPort: 8000\n",
    "      name: http\n",
    "    resources:\n",
    "      requests:\n",
    "        memory: \"4Gi\"\n",
    "      limits:\n",
    "        memory: \"8Gi\"\n",
    "  restartPolicy: Never\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Webhook Injection\n",
    "\n",
    "Check that the webhook successfully injected the profiler configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Environment variables:\"\n",
    "oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[0].env[*].name}' | tr ' ' '\\n' | grep -E 'PYTHON|VLLM' || echo \"  (waiting for pod to be created...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Volume mounts:\"\n",
    "oc get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.containers[0].volumeMounts[*].mountPath}' | tr ' ' '\\n' | grep profiler || echo \"  (waiting for pod to be created...)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Wait for vLLM Server to Start\n",
    "\n",
    "Wait for the pod to be running and the vLLM server to be ready to accept requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for pod to be running (may take a few minutes)\n",
    "!oc wait --for=condition=Ready pod/{POD_NAME} -n {NAMESPACE} --timeout=300s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for profiler installation in logs\n",
    "!echo \"Checking if profiler was loaded...\"\n",
    "!oc logs {POD_NAME} -n {NAMESPACE} 2>&1 | grep -E '\\[profiler\\]' | head -5 || echo \"Profiler messages not yet visible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for vLLM server to respond to /v1/models endpoint\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "print(\"Waiting for vLLM server to be ready (checking /v1/models endpoint)...\")\n",
    "max_wait = 600  # 10 minutes\n",
    "elapsed = 0\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    result = subprocess.run(\n",
    "        f\"oc exec {POD_NAME} -n {NAMESPACE} -- curl -sf http://localhost:8000/v1/models\",\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\n✓ vLLM server is ready! (took {elapsed}s)\")\n",
    "        break\n",
    "    \n",
    "    if elapsed >= max_wait:\n",
    "        print(f\"\\n✗ Timeout waiting for server to start\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)\n",
    "    elapsed += 5\n",
    "    if elapsed % 30 == 0:\n",
    "        print(f\"  Still waiting... ({elapsed}s elapsed)\")\n",
    "    else:\n",
    "        print(\".\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pod IP for reference\n",
    "POD_IP = !oc get pod {POD_NAME} -n {NAMESPACE} -o jsonpath='{{.status.podIP}}'\n",
    "POD_IP = POD_IP[0]\n",
    "os.environ[\"POD_IP\"] = POD_IP\n",
    "print(f\"Pod IP: {POD_IP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Send Inference Request\n",
    "\n",
    "Send a single inference request that generates 200 tokens. This will trigger the profiler (configured for calls 100-150)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -x\n",
    "# Create temporary client pod to send request\n",
    "echo \"Sending inference request to generate 200 tokens...\"\n",
    "\n",
    "oc run demo-curl-client -n $NAMESPACE --image=curlimages/curl:latest --rm -i --restart=Never -- /bin/sh -c \"\n",
    "set -e\n",
    "echo 'Sending inference request...'\n",
    "curl -X POST http://$POD_IP:8000/v1/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{\n",
    "        \\\"model\\\": \\\"$MODEL\\\",\n",
    "        \\\"prompt\\\": \\\"Write a detailed story about a brave knight who goes on an adventure:\\\",\n",
    "        \\\"max_tokens\\\": 200,\n",
    "        \\\"temperature\\\": 0.7\n",
    "    }'\n",
    "echo ''\n",
    "echo 'Request completed successfully'\n",
    "\" || echo \"Request may have failed, continuing to check logs...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Profiler Output\n",
    "\n",
    "Check the pod logs to see the profiler output with CPU and CUDA timing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a few seconds for profiler to write output\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for profiler installation message\n",
    "!echo \"=== Profiler Installation ===\"\n",
    "!oc logs {POD_NAME} -n {NAMESPACE} 2>&1 | grep \"\\[profiler\\] vLLM profiler installed\" || echo \"✗ Profiler not loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for profiler start message\n",
    "!echo \"=== Profiler Start ===\"\n",
    "!oc logs {POD_NAME} -n {NAMESPACE} 2>&1 | grep \"\\[profiler\\] Starting profiler\" || echo \"△ Profiler start message not found (may need more requests)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract profiler output (first 60 lines)\n",
    "!echo \"=== Profiler Output (Top Operations by CUDA Time) ===\"\n",
    "!oc logs {POD_NAME} -n {NAMESPACE} 2>&1 | sed -n '/===== begin profiler output/,/===== end profiler output/p' | head -60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for trace export message\n",
    "!echo \"=== Trace Export Status ===\"\n",
    "!oc logs {POD_NAME} -n {NAMESPACE} 2>&1 | grep -E \"Exported trace to:|Chrome trace export disabled\" | tail -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Optional - Retrieve Trace File\n",
    "\n",
    "If trace export is enabled, retrieve the Chrome trace JSON file for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Trace files in pod:\"\n",
    "#use sh -c otherwise jupyter will single quote the wildcard expansion\n",
    "oc exec $POD_NAME -n $NAMESPACE -- /bin/sh -c \"ls -lh /tmp/trace*.json 2>/dev/null\" || echo \"No trace files found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve trace file (if exists)\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Get trace filename from pod\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        \"oc\", \"exec\", POD_NAME, \"-n\", NAMESPACE,\n",
    "        \"--\", \"/bin/sh\", \"-c\", \"ls /tmp/trace*.json 2>/dev/null\"\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    trace_file = result.stdout.strip().split('\\n')[0]\n",
    "    local_trace = \"./demo-trace.json\"\n",
    "    \n",
    "    copy_result = subprocess.run(\n",
    "        f\"oc cp {NAMESPACE}/{POD_NAME}:{trace_file} {local_trace}\",\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    if copy_result.returncode == 0:\n",
    "        print(f\"✓ Trace file retrieved: {local_trace}\")\n",
    "        print(f\"  Size: {os.path.getsize(local_trace)} bytes\")\n",
    "        print(f\"\\nTo visualize:\")\n",
    "        print(f\"  1. Open Chrome browser\")\n",
    "        print(f\"  2. Navigate to chrome://tracing\")\n",
    "        print(f\"  3. Click 'Load' and select {local_trace}\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to copy trace file\")\n",
    "else:\n",
    "    print(\"No trace files found (trace export may be disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup\n",
    "\n",
    "Remove the test pod (keep the profiler webhook deployed for future use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the demo pod\n",
    "#!oc delete pod {POD_NAME} -n {NAMESPACE} --ignore-not-found=true\n",
    "#print(f\"\\n✓ Demo pod {POD_NAME} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, delete the entire profiler system\n",
    "# Uncomment the line below to remove webhook and ConfigMap\n",
    "\n",
    "# !./teardown.sh --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo showed the complete vLLM profiler workflow:\n",
    "\n",
    "✓ **Deployed** mutating webhook and ConfigMap  \n",
    "✓ **Created** vLLM pod with automatic profiler instrumentation  \n",
    "✓ **Verified** webhook injected PYTHONPATH and mounted profiler code  \n",
    "✓ **Sent** inference request to trigger profiling  \n",
    "✓ **Captured** profiler output with CPU/CUDA timing data  \n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "- **Zero Code Changes**: vLLM source code unchanged\n",
    "- **Transparent Injection**: Webhook automatically instruments matching pods\n",
    "- **Import Hook Magic**: sitecustomize.py wraps Worker.execute_model at import time\n",
    "- **Configurable Ranges**: Profile specific call ranges (e.g., 100-150)\n",
    "- **Multi-source Config**: Environment variables override YAML config\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Customize profiling ranges via annotations: `vllm.profiler/ranges=\"50-100,200-300\"`\n",
    "- Enable trace export: `vllm.profiler/export-trace=\"true\"`\n",
    "- Visualize traces in Chrome: `chrome://tracing`\n",
    "- Profile production workloads: Add label `llm-d.ai/inferenceServing=true` to any vLLM pod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
