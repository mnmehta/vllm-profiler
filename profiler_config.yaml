# vLLM Profiler Configuration
# This file controls torch profiler behavior for instrumented vLLM pods

# Profiling ranges - specify which model execution calls to profile
# Format: "start-end" or "start1-end1,start2-end2,..." for multiple ranges
# Examples:
#   "100-150"           - Profile calls 100 through 150
#   "50-100,200-300"    - Profile calls 50-100 AND 200-300
#   "0-50"              - Profile first 50 calls
profiling_ranges: "100-150"

# Torch profiler activities to record
# Available: CPU, CUDA
# Use comma-separated list: "CPU,CUDA"
activities: "CPU,CUDA"

# Profiler options
options:
  # Record tensor shapes in trace
  record_shapes: true

  # Capture Python stack traces (useful for debugging but adds overhead)
  with_stack: true

  # Profile memory allocations (adds overhead)
  profile_memory: false

  # Record function names for better trace readability
  with_modules: false

# Output configuration
output:
  # Export Chrome trace JSON file (can be opened in chrome://tracing)
  # Set to false to skip trace export (only print table)
  export_chrome_trace: true

  # Output file path (supports environment variable substitution)
  # Use {pid} as placeholder for process ID
  # Use {rank} for tensor parallel rank (if available)
  file_pattern: "trace_pid{pid}.json"

  # Table output settings
  table:
    enabled: true
    sort_by: "cuda_time_total"  # Options: cuda_time_total, cpu_time_total, cpu_time, cuda_time
    row_limit: 50

  # Print profiler statistics to logs
  print_stats: true

# Advanced settings
advanced:
  # Module to intercept (change only if using different vLLM version)
  target_module: "vllm.v1.worker.gpu_worker"

  # Class and method to wrap
  target_class: "Worker"
  target_method: "execute_model"

  # Enable debug logging from profiler
  debug: false
