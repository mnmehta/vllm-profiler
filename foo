/opt/vllm/lib64/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
========= Using hotreload module pid 1 pidname vllm PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
DEBUG 12-05 01:03:18 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 12-05 01:03:18 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 12-05 01:03:18 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 12-05 01:03:18 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 12-05 01:03:18 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:18 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 12-05 01:03:18 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 12-05 01:03:24 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 12-05 01:03:24 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 12-05 01:03:24 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1)[0;0m INFO 12-05 01:03:24 [entrypoints/openai/api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1)[0;0m INFO 12-05 01:03:24 [entrypoints/utils.py:233] non-default args: {'model_tag': 'Qwen/Qwen3-0.6B', 'port': 8200, 'disable_uvicorn_access_log': True, 'max_model_len': 32000, 'served_model_name': ['Qwen/Qwen3-0.6B'], 'tensor_parallel_size': 4, 'block_size': 128, 'kv_transfer_config': KVTransferConfig(kv_connector='NixlConnector', engine_id='deef9af4-d138-44d0-9a88-2c381f9d02b5', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None)}
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:25 [model_executor/models/registry.py:450] Cached model info file for class vllm.model_executor.models.qwen3.Qwen3ForCausalLM not found
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:25 [model_executor/models/registry.py:503] Cache model info for class vllm.model_executor.models.qwen3.Qwen3ForCausalLM miss. Loading model instead.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:36 [model_executor/models/registry.py:511] Loaded model info for class vllm.model_executor.models.qwen3.Qwen3ForCausalLM
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:36 [logging_utils/log_time.py:27] Registry inspect model class: Elapsed time 11.3649467 secs
[1;36m(APIServer pid=1)[0;0m INFO 12-05 01:03:36 [config/model.py:547] Resolved architecture: Qwen3ForCausalLM
[1;36m(APIServer pid=1)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1)[0;0m INFO 12-05 01:03:36 [config/model.py:1510] Using max model len 32000
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:36 [engine/arg_utils.py:1672] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:36 [engine/arg_utils.py:1681] Setting max_num_seqs to 1024 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:36 [config/parallel.py:479] Defaulting to use mp for distributed inference
[1;36m(APIServer pid=1)[0;0m INFO 12-05 01:03:36 [config/scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
========= Using hotreload module pid 266 pidname python3 PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
/opt/vllm/lib64/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
========= Using hotreload module pid 267 pidname python3 PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
DEBUG 12-05 01:03:43 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 12-05 01:03:43 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 12-05 01:03:43 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 12-05 01:03:43 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 12-05 01:03:43 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:43 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 12-05 01:03:43 [platforms/__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:47 [v1/engine/utils.py:772] Waiting for 1 local, 0 remote core engine proc(s) to connect.
[1;36m(EngineCore_DP0 pid=267)[0;0m INFO 12-05 01:03:49 [v1/engine/core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:49 [v1/engine/utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:03:49 [v1/engine/core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/f7a15805-57ec-41de-a756-f8d5b1ff0f71'], outputs=['ipc:///tmp/094be894-82b4-4a5e-80d9-87f5ffbd1437'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:03:49 [v1/engine/core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:03:49 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:03:49 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:03:49 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=267)[0;0m INFO 12-05 01:03:49 [v1/engine/core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=267)[0;0m WARNING 12-05 01:03:49 [v1/executor/multiproc_executor.py:720] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:03:49 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/da1d3244-f7ce-41f5-8327-6f226baae209
[1;36m(EngineCore_DP0 pid=267)[0;0m INFO 12-05 01:03:49 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_550b5e9f'), local_subscribe_addr='ipc:///tmp/da1d3244-f7ce-41f5-8327-6f226baae209', remote_subscribe_addr=None, remote_addr_ipv6=False)
/opt/vllm/lib64/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/vllm/lib64/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/vllm/lib64/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/vllm/lib64/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
========= Using hotreload module pid 406 pidname python3 PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
DEBUG 12-05 01:03:55 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 12-05 01:03:55 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
========= Using hotreload module pid 403 pidname python3 PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
DEBUG 12-05 01:03:55 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 12-05 01:03:55 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
========= Using hotreload module pid 404 pidname python3 PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
DEBUG 12-05 01:03:55 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 12-05 01:03:55 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 12-05 01:03:55 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 12-05 01:03:55 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 12-05 01:03:55 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 12-05 01:03:55 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 12-05 01:03:55 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 12-05 01:03:55 [platforms/__init__.py:216] Automatically detected platform cuda.
INFO 12-05 01:03:55 [platforms/__init__.py:216] Automatically detected platform cuda.
INFO 12-05 01:03:55 [platforms/__init__.py:216] Automatically detected platform cuda.
========= Using hotreload module pid 405 pidname python3 PYTHONPATH /home/vllm/hotreload
[sitecustomize] sys.path:
  0: /home/vllm/hotreload
  1: /usr/lib64/python312.zip
  2: /usr/lib64/python3.12
  3: /usr/lib64/python3.12/lib-dynload
  4: /opt/vllm/lib64/python3.12/site-packages
  5: __editable__.vllm-0.11.0+precompiled.finder.__path_hook__
  6: /opt/vllm/lib/python3.12/site-packages
[sitecustomize] sitecustomize completed, sys.path and PYTHONPATH cleaned up.
DEBUG 12-05 01:03:55 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 12-05 01:03:55 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 12-05 01:03:55 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 12-05 01:03:55 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 12-05 01:03:55 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 12-05 01:03:55 [platforms/__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:03:59 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 12-05 01:04:00 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 12-05 01:04:00 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 12-05 01:04:00 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 12-05 01:04:00 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 12-05 01:04:00 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 12-05 01:04:00 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 12-05 01:04:00 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 12-05 01:04:00 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 12-05 01:04:00 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 12-05 01:04:01 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 12-05 01:04:01 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 12-05 01:04:01 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 12-05 01:04:01 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:01 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:01 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:01 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 12-05 01:04:03 [compilation/decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 12-05 01:04:03 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f193bfbc1d0>
DEBUG 12-05 01:04:03 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa6d15d2f30>
DEBUG 12-05 01:04:03 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fbeac60f290>
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/da1d3244-f7ce-41f5-8327-6f226baae209
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/da1d3244-f7ce-41f5-8327-6f226baae209
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/da1d3244-f7ce-41f5-8327-6f226baae209
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/b7db9f5e-bb3d-4fa4-bd7d-40481912caf0
INFO 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b4e299ff'), local_subscribe_addr='ipc:///tmp/b7db9f5e-bb3d-4fa4-bd7d-40481912caf0', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 12-05 01:04:03 [utils/__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb852404200>
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/da1d3244-f7ce-41f5-8327-6f226baae209
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/23dfd922-439c-4bfb-84a8-86e5f422909e
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/33d7ea98-4ec1-4efc-90be-6ca8cd69a465
DEBUG 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/0fa75e13-eb12-4c77-bc91-799aa1068e15
INFO 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_966017f5'), local_subscribe_addr='ipc:///tmp/33d7ea98-4ec1-4efc-90be-6ca8cd69a465', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_725f72d8'), local_subscribe_addr='ipc:///tmp/23dfd922-439c-4bfb-84a8-86e5f422909e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-05 01:04:03 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0e73cac0'), local_subscribe_addr='ipc:///tmp/0fa75e13-eb12-4c77-bc91-799aa1068e15', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 12-05 01:04:04 [distributed/parallel_state.py:1029] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:39241 backend=nccl
DEBUG 12-05 01:04:04 [distributed/parallel_state.py:1029] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39241 backend=nccl
DEBUG 12-05 01:04:04 [distributed/parallel_state.py:1029] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:39241 backend=nccl
DEBUG 12-05 01:04:04 [distributed/parallel_state.py:1029] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:39241 backend=nccl
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
DEBUG 12-05 01:04:05 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 12-05 01:04:05 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 12-05 01:04:05 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
DEBUG 12-05 01:04:05 [distributed/parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-05 01:04:05 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:05 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:05 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:05 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:05 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:05 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:05 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:05 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:06 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 12-05 01:04:06 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 12-05 01:04:06 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 12-05 01:04:06 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
DEBUG 12-05 01:04:06 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/7c06d6bd-b9e0-4c6b-94d4-20af4896b8cc
INFO 12-05 01:04:06 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ed2a19e0'), local_subscribe_addr='ipc:///tmp/7c06d6bd-b9e0-4c6b-94d4-20af4896b8cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 12-05 01:04:06 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/7c06d6bd-b9e0-4c6b-94d4-20af4896b8cc
DEBUG 12-05 01:04:06 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/7c06d6bd-b9e0-4c6b-94d4-20af4896b8cc
DEBUG 12-05 01:04:06 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/7c06d6bd-b9e0-4c6b-94d4-20af4896b8cc
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-05 01:04:06 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:06 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:06 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:06 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:06 [utils/__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-05 01:04:06 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:06 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:06 [distributed/device_communicators/pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-05 01:04:06 [distributed/parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-05 01:04:06 [distributed/parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-05 01:04:06 [distributed/parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-05 01:04:06 [distributed/parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:56] NIXL is available
INFO 12-05 01:04:06 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../kv_connector/factory.py:51] Creating v1 connector with name: NixlConnector and engine_id: deef9af4-d138-44d0-9a88-2c381f9d02b5
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:465] Initializing NIXL wrapper
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:466] Initializing NIXL worker deef9af4-d138-44d0-9a88-2c381f9d02b5
E1205 01:04:06.700713     406 nixl_plugin_manager.cpp:122] Failed to load plugin from /opt/vllm/lib64/python3.12/site-packages/.nixl.mesonpy.libs/plugins/libplugin_LIBFABRIC.so: libfabric.so.1: cannot open shared object file: No such file or directory
E1205 01:04:06.700719     403 nixl_plugin_manager.cpp:122] Failed to load plugin from /opt/vllm/lib64/python3.12/site-packages/.nixl.mesonpy.libs/plugins/libplugin_LIBFABRIC.so: libfabric.so.1: cannot open shared object file: No such file or directory
E1205 01:04:06.700717     404 nixl_plugin_manager.cpp:122] Failed to load plugin from /opt/vllm/lib64/python3.12/site-packages/.nixl.mesonpy.libs/plugins/libplugin_LIBFABRIC.so: libfabric.so.1: cannot open shared object file: No such file or directory
E1205 01:04:06.700747     403 nixl_plugin_manager.cpp:288] Failed to load plugin 'LIBFABRIC' from any directory
E1205 01:04:06.700747     404 nixl_plugin_manager.cpp:288] Failed to load plugin 'LIBFABRIC' from any directory
E1205 01:04:06.700747     406 nixl_plugin_manager.cpp:288] Failed to load plugin 'LIBFABRIC' from any directory
E1205 01:04:06.701073     405 nixl_plugin_manager.cpp:122] Failed to load plugin from /opt/vllm/lib64/python3.12/site-packages/.nixl.mesonpy.libs/plugins/libplugin_LIBFABRIC.so: libfabric.so.1: cannot open shared object file: No such file or directory
E1205 01:04:06.701107     405 nixl_plugin_manager.cpp:288] Failed to load plugin 'LIBFABRIC' from any directory
2025-12-05 01:04:06 NIXL INFO    _api.py:361 Backend UCX was instantiated
2025-12-05 01:04:06 NIXL INFO    _api.py:251 Initialized NIXL agent: c9980d13-432c-45d1-913e-94fe479aa3f7
INFO 12-05 01:04:06 [platforms/cuda.py:366] Using Flash Attention backend on V1 engine.
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:166] NixlConnector setting KV cache layout to HND for better xfer performance.
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASH_ATTN
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout HND
2025-12-05 01:04:06 NIXL INFO    _api.py:361 Backend UCX was instantiated
2025-12-05 01:04:06 NIXL INFO    _api.py:251 Initialized NIXL agent: 06ac5e9c-a4c5-4b46-a425-f29e2dab43b9
INFO 12-05 01:04:06 [platforms/cuda.py:366] Using Flash Attention backend on V1 engine.
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:166] NixlConnector setting KV cache layout to HND for better xfer performance.
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASH_ATTN
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout HND
2025-12-05 01:04:06 NIXL INFO    _api.py:361 Backend UCX was instantiated
2025-12-05 01:04:06 NIXL INFO    _api.py:251 Initialized NIXL agent: 9f871294-41fa-4687-b7c2-52fe8a5016de
INFO 12-05 01:04:06 [platforms/cuda.py:366] Using Flash Attention backend on V1 engine.
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:166] NixlConnector setting KV cache layout to HND for better xfer performance.
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASH_ATTN
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout HND
2025-12-05 01:04:06 NIXL INFO    _api.py:361 Backend UCX was instantiated
2025-12-05 01:04:06 NIXL INFO    _api.py:251 Initialized NIXL agent: b6989291-fecd-49c3-b924-c58180350f03
INFO 12-05 01:04:06 [platforms/cuda.py:366] Using Flash Attention backend on V1 engine.
INFO 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:166] NixlConnector setting KV cache layout to HND for better xfer performance.
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:597] Detected attention backend FLASH_ATTN
DEBUG 12-05 01:04:06 [distributed/.../v1/nixl_connector.py:598] Detected kv cache layout HND
INFO 12-05 01:04:07 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
INFO 12-05 01:04:07 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
INFO 12-05 01:04:07 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
DEBUG 12-05 01:04:07 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
DEBUG 12-05 01:04:07 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
DEBUG 12-05 01:04:07 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
INFO 12-05 01:04:07 [v1/sample/ops/topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.
DEBUG 12-05 01:04:07 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-0.6B...
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-0.6B...
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-0.6B...
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-0.6B...
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:07 [v1/worker/gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:649] enabled custom ops: Counter()
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:649] enabled custom ops: Counter()
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:650] disabled custom ops: Counter({'rms_norm': 113, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:650] disabled custom ops: Counter({'rms_norm': 113, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:649] enabled custom ops: Counter()
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:650] disabled custom ops: Counter({'rms_norm': 113, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:07 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:07 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:07 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:07 [compilation/backends.py:42] Using InductorAdaptor
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:649] enabled custom ops: Counter()
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:07 [config/compilation.py:650] disabled custom ops: Counter({'rms_norm': 113, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:07 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:07 [model_executor/model_loader/weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:07 [model_executor/model_loader/weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:07 [model_executor/model_loader/weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:07 [model_executor/model_loader/weight_utils.py:450] No model.safetensors.index.json found in remote.
[1;36m(Worker_TP0 pid=403)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:07 [model_executor/model_loader/weight_utils.py:450] No model.safetensors.index.json found in remote.
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:07 [model_executor/model_loader/weight_utils.py:450] No model.safetensors.index.json found in remote.
[1;36m(Worker_TP0 pid=403)[0;0m Exception in thread Thread-2 (_report_usage_worker):
[1;36m(Worker_TP0 pid=403)[0;0m Traceback (most recent call last):
[1;36m(Worker_TP0 pid=403)[0;0m   File "/usr/lib64/python3.12/threading.py", line 1075, in _bootstrap_inner
[1;36m(Worker_TP0 pid=403)[0;0m     self.run()
[1;36m(Worker_TP0 pid=403)[0;0m   File "/usr/lib64/python3.12/threading.py", line 1012, in run
[1;36m(Worker_TP0 pid=403)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(Worker_TP0 pid=403)[0;0m   File "/opt/vllm-source/vllm/usage/usage_lib.py", line 167, in _report_usage_worker
[1;36m(Worker_TP0 pid=403)[0;0m     self._report_usage_once(model_architecture, usage_context, extra_kvs)
[1;36m(Worker_TP0 pid=403)[0;0m   File "/opt/vllm-source/vllm/usage/usage_lib.py", line 195, in _report_usage_once
[1;36m(Worker_TP0 pid=403)[0;0m     info = cpuinfo.get_cpu_info()
[1;36m(Worker_TP0 pid=403)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_TP0 pid=403)[0;0m   File "/opt/vllm/lib64/python3.12/site-packages/cpuinfo/cpuinfo.py", line 2762, in get_cpu_info
[1;36m(Worker_TP0 pid=403)[0;0m     output = json.loads(output, object_hook = _utf_to_str)
[1;36m(Worker_TP0 pid=403)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_TP0 pid=403)[0;0m   File "/usr/lib64/python3.12/json/__init__.py", line 359, in loads
[1;36m(Worker_TP0 pid=403)[0;0m     return cls(**kw).decode(s)
[1;36m(Worker_TP0 pid=403)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_TP0 pid=403)[0;0m   File "/usr/lib64/python3.12/json/decoder.py", line 338, in decode
[1;36m(Worker_TP0 pid=403)[0;0m     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[1;36m(Worker_TP0 pid=403)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(Worker_TP0 pid=403)[0;0m   File "/usr/lib64/python3.12/json/decoder.py", line 356, in raw_decode
[1;36m(Worker_TP0 pid=403)[0;0m     raise JSONDecodeError("Expecting value", s, err.value) from None
[1;36m(Worker_TP0 pid=403)[0;0m json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:04:09 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:09 [model_executor/model_loader/weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:09 [model_executor/model_loader/weight_utils.py:413] Time spent downloading weights for Qwen/Qwen3-0.6B: 0.586226 seconds
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:10 [model_executor/model_loader/weight_utils.py:450] No model.safetensors.index.json found in remote.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:04:19 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:04:29 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(Worker_TP0 pid=403)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.21s/it]
[1;36m(Worker_TP0 pid=403)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.21s/it]
[1;36m(Worker_TP0 pid=403)[0;0m 
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:32 [model_executor/model_loader/default_loader.py:267] Loading weights took 24.17 seconds
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:32 [model_executor/model_loader/default_loader.py:267] Loading weights took 24.22 seconds
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:32 [model_executor/model_loader/default_loader.py:267] Loading weights took 21.61 seconds
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:32 [model_executor/model_loader/default_loader.py:267] Loading weights took 24.05 seconds
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:32 [v1/worker/gpu_model_runner.py:2653] Model loading took 0.2922 GiB and 24.472843 seconds
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:04:32 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/23dfd922-439c-4bfb-84a8-86e5f422909e
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:32 [v1/worker/gpu_model_runner.py:2653] Model loading took 0.2922 GiB and 24.508095 seconds
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:04:32 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/0fa75e13-eb12-4c77-bc91-799aa1068e15
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:32 [v1/worker/gpu_model_runner.py:2653] Model loading took 0.2922 GiB and 24.502496 seconds
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:32 [v1/worker/gpu_model_runner.py:2653] Model loading took 0.2922 GiB and 24.513782 seconds
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:04:32 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/33d7ea98-4ec1-4efc-90be-6ca8cd69a465
[1;36m(EngineCore_DP0 pid=267)[0;0m DEBUG 12-05 01:04:32 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/b7db9f5e-bb3d-4fa4-bd7d-40481912caf0
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:32 [compilation/decorators.py:256] Start compiling function <code object forward at 0x5575b4261ab0, file "/opt/vllm-source/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:32 [compilation/decorators.py:256] Start compiling function <code object forward at 0x56013b941480, file "/opt/vllm-source/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:32 [compilation/decorators.py:256] Start compiling function <code object forward at 0x560220283530, file "/opt/vllm-source/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:32 [compilation/decorators.py:256] Start compiling function <code object forward at 0x5592207bef60, file "/opt/vllm-source/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/attention/layer.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/communication_op.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/parallel_state.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/custom_op.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/activation.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/layernorm.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/linear.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/utils.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen2.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen3.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/platforms/interface.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/nn/modules/container.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/attention/layer.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/communication_op.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/parallel_state.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/custom_op.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/activation.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/layernorm.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/linear.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/utils.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen2.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen3.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/platforms/interface.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/nn/modules/container.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/attention/layer.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/communication_op.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/parallel_state.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/custom_op.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/activation.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/layernorm.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/linear.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/utils.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen2.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen3.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/platforms/interface.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/nn/modules/container.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/attention/layer.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/communication_op.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/distributed/parallel_state.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/custom_op.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/activation.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/layernorm.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/linear.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/utils.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen2.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/model_executor/models/qwen3.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm-source/vllm/platforms/interface.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:37 [compilation/backends.py:501] /opt/vllm/lib64/python3.12/site-packages/torch/nn/modules/container.py
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:548] Using cache directory: /tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:559] Dynamo bytecode transform time: 5.20 s
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:548] Using cache directory: /tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:548] Using cache directory: /tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:559] Dynamo bytecode transform time: 5.24 s
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:559] Dynamo bytecode transform time: 5.24 s
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:548] Using cache directory: /tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:38 [compilation/backends.py:559] Dynamo bytecode transform time: 5.26 s
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:04:39 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:39 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:39 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:39 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:39 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:39 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:42 [compilation/backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:42 [compilation/backends.py:203] Store the 0-th graph for dynamic shape from inductor via handle ('fbqbj5cqpvshbecequk7urgklcoyqxfioghgefefpdttj6xeyuyd', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/in/cin45affn2yu6ho2dx62ofrqmzslylhyxywieegfififitin7el2.py')
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:42 [compilation/backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:42 [compilation/backends.py:203] Store the 0-th graph for dynamic shape from inductor via handle ('fqna7p6xlxjvg2ksqvv744dibcfqw34utb53xvivusx7rsdasyjm', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/ub/cubt5so46no6bb6nwmivsyedpwocreehgirpv35rnvh5wzzrgorc.py')
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:42 [compilation/backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:42 [compilation/backends.py:203] Store the 0-th graph for dynamic shape from inductor via handle ('fmeyde4cucdfjr4p6snclkogjn7nosfefthgnqwvjwclbazjmpv5', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/h3/ch3hr7rplq3zlhzjkzutivcx6m5r54z7jjlann2pnlyso3v2f57q.py')
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:42 [compilation/backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:42 [compilation/backends.py:203] Store the 0-th graph for dynamic shape from inductor via handle ('fmrc7xkzu3vxrcmbfhma4lpeuxzqil5kme7m6y723ztwp65wnfig', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/pt/cptdt2nb5c3jw6yjz2nxex5p6eteeguspuunygtb5zf6w5kgsfbc.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:42 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:42 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.3 ms
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:42 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.4 ms
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:42 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:42 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:44 [compilation/backends.py:203] Store the 1-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:44 [compilation/backends.py:203] Store the 1-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:44 [compilation/backends.py:203] Store the 1-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:44 [compilation/backends.py:203] Store the 1-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 2-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 2-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 2-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 2-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 3-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 3-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 3-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:45 [compilation/backends.py:203] Store the 3-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:46 [compilation/backends.py:203] Store the 4-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:46 [compilation/backends.py:203] Store the 4-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:46 [compilation/backends.py:203] Store the 4-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:46 [compilation/backends.py:203] Store the 4-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 5-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 5-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 5-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 5-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 6-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 6-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 6-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:47 [compilation/backends.py:203] Store the 6-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 7-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 7-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 7-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 7-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 8-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 8-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 8-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:48 [compilation/backends.py:203] Store the 8-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 9-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 9-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 9-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:04:49 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 9-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 10-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 10-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 10-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 10-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 11-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 11-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:49 [compilation/backends.py:203] Store the 11-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 11-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 12-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 12-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 12-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 12-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 13-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 13-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:50 [compilation/backends.py:203] Store the 13-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 13-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 14-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 14-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 14-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 14-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 15-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 15-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 15-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:51 [compilation/backends.py:203] Store the 15-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 16-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 16-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 16-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 16-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 17-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 17-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 17-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:52 [compilation/backends.py:203] Store the 17-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:53 [compilation/backends.py:203] Store the 18-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:53 [compilation/backends.py:203] Store the 18-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:53 [compilation/backends.py:203] Store the 18-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:53 [compilation/backends.py:203] Store the 18-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 19-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 19-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 19-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 19-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 20-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 20-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 20-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 20-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:54 [compilation/backends.py:203] Store the 21-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 21-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 21-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 21-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 22-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 22-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 22-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 22-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 23-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:55 [compilation/backends.py:203] Store the 23-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 23-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 23-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 24-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 24-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 24-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 25-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 24-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 25-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:56 [compilation/backends.py:203] Store the 25-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 26-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 25-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 26-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 26-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 27-th graph for dynamic shape from inductor via handle ('fbxdelaw4kv3voxbio4thgoc66cfhpske7wq54cf7k3rrrj5f2hq', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/ji/cjissfzaicankqziplafhmjh6wlx5p3itd6k7lvfxvlw5pkqj4m7.py')
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 26-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:57 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:57 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:57 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 27-th graph for dynamic shape from inductor via handle ('f7vmfnx7uzs76f46cmg5wd62d6garquaq6umwsibmusyz3hlvn33', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/lq/clqkdixhdmrrrcmb5jmv44skxvygv3kyanqzbdcxnvagca7fqco6.py')
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:57 [compilation/backends.py:203] Store the 27-th graph for dynamic shape from inductor via handle ('f5zoqzwipw46vpcbv756eshj2z3l5ndwat3745vd4l77qb2aacho', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ju/cjuns4kblqlgm22zbet6ioy4hcpsxqus3mz6ds3oeqnpnz2syzxk.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:203] Store the 28-th graph for dynamic shape from inductor via handle ('fzjvejkcogv4q2xajac6qvamluoagugsvl6tnvtlsbthpzwvcrpt', '/tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/inductor_cache/27/c27lelunzxtpdlrdzxegztpku25uiod7mfj6spgidbsk4zznazxe.py')
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:04:58 [compilation/backends.py:218] Compiling a graph for dynamic shape takes 19.42 s
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:58 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:58 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:58 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:58 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:58 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:58 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:203] Store the 27-th graph for dynamic shape from inductor via handle ('fx4ezqpylhypjlzy6msvavtmz2mtln2e4cwohniz6v2qhmsgqo3g', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/3t/c3tfspzjeqbfqcxoimp25o2yit3bvmxwpjoopdwa4i4kd3kwjtqa.py')
[1;36m(Worker_TP3 pid=406)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:602] Computation graph saved to /tmp/vllm/torch_compile_cache/77c10de462/rank_3_0/backbone/computation_graph.py
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:203] Store the 28-th graph for dynamic shape from inductor via handle ('fmq7jksmmqmla77w7bdft7jfca4nvma2db56ftgzaujsm7trixg3', '/tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/inductor_cache/gp/cgpxcnomgkatqelvb4hvcahfhpouy6iamgwj5qutcw4jooyv3lph.py')
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:04:58 [compilation/backends.py:218] Compiling a graph for dynamic shape takes 19.68 s
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:203] Store the 28-th graph for dynamic shape from inductor via handle ('fgpsqqjc5kwulvvy7g2bxgbotx5tm24jrk3o2exfzkthkft7lwe3', '/tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/inductor_cache/ge/cget2xhdbxa74b7jqbtmgwnyk2otyjc5wanxmhmwwlz4zjphuh2d.py')
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:04:58 [compilation/backends.py:218] Compiling a graph for dynamic shape takes 19.71 s
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:58 [compilation/vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:58 [compilation/fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:58 [compilation/vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.2 ms
[1;36m(Worker_TP0 pid=403)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:602] Computation graph saved to /tmp/vllm/torch_compile_cache/77c10de462/rank_0_0/backbone/computation_graph.py
[1;36m(Worker_TP2 pid=405)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:602] Computation graph saved to /tmp/vllm/torch_compile_cache/77c10de462/rank_2_0/backbone/computation_graph.py
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:203] Store the 28-th graph for dynamic shape from inductor via handle ('fsft4yzn6o4jjqgdvbdoavvphgzmblcevju7zm7hpazit6t3nunx', '/tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/inductor_cache/pt/cptyo6sl7gzn4mzhuyz4h3qmpy2la6fug4bmf4urcibuyjp3cqet.py')
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:04:58 [compilation/backends.py:218] Compiling a graph for dynamic shape takes 19.95 s
[1;36m(Worker_TP1 pid=404)[0;0m DEBUG 12-05 01:04:58 [compilation/backends.py:602] Computation graph saved to /tmp/vllm/torch_compile_cache/77c10de462/rank_1_0/backbone/computation_graph.py
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:04:59 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(Worker_TP3 pid=406)[0;0m INFO 12-05 01:05:02 [compilation/monitor.py:34] torch.compile takes 24.68 s in total
[1;36m(Worker_TP1 pid=404)[0;0m INFO 12-05 01:05:02 [compilation/monitor.py:34] torch.compile takes 25.19 s in total
[1;36m(Worker_TP2 pid=405)[0;0m INFO 12-05 01:05:02 [compilation/monitor.py:34] torch.compile takes 24.91 s in total
[1;36m(Worker_TP0 pid=403)[0;0m INFO 12-05 01:05:02 [compilation/monitor.py:34] torch.compile takes 24.92 s in total
[1;36m(APIServer pid=1)[0;0m DEBUG 12-05 01:05:09 [v1/engine/utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
